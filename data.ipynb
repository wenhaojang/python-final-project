{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "415dafab",
   "metadata": {},
   "source": [
    "处理路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38748833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR = C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def find_project_root(start=None, markers=(\"整合数据\", \"原始数据\", \"README.md\")):\n",
    "    \"\"\"\n",
    "    从 start 目录开始向上查找，直到找到包含 markers 中任一标记文件/文件夹的目录，作为项目根目录。\n",
    "    \"\"\"\n",
    "    cur = Path(start or os.getcwd()).resolve()\n",
    "    for p in [cur] + list(cur.parents):\n",
    "        if any((p / m).exists() for m in markers):\n",
    "            return p\n",
    "    raise RuntimeError(f\"Cannot find project root from {cur}. Please check markers or working dir.\")\n",
    "\n",
    "BASE_DIR = find_project_root()\n",
    "print(\"BASE_DIR =\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82351b",
   "metadata": {},
   "source": [
    "# 处理Y的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb911eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成： C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\etf_y_slog_wide.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\etf_y_slog_long.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17514\\AppData\\Local\\Temp\\ipykernel_30184\\211647137.py:78: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  long = long.groupby(\"etf_name\", group_keys=False).apply(_clip_grp)\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas numpy scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===== 1) 读入原始数据 =====\n",
    "# 把路径换成你的文件\n",
    "input_path = BASE_DIR /\"原始数据\"/\"各行业etf资金流入（被解释变量）1.xlsx\"   # 示例：CSV；若是 Excel，见下方注释\n",
    "df_raw = pd.read_excel(input_path, sheet_name=0)  # 如果是 xlsx\n",
    "\n",
    "\n",
    "# 统一列名（去空格）\n",
    "df_raw.columns = [str(c).strip() for c in df_raw.columns]\n",
    "\n",
    "# 定位“代码”行（第二行），保存代码映射后删除该行\n",
    "code_row_idx = None\n",
    "for i in range(min(5, len(df_raw))):\n",
    "    if str(df_raw.iloc[i,0]).strip() in [\"代码\", \"基金代码\", \"代码/基金\"]:\n",
    "        code_row_idx = i\n",
    "        break\n",
    "\n",
    "code_map = {}\n",
    "if code_row_idx is not None:\n",
    "    # 第一列是“代码”字样，其余列为代码\n",
    "    for col in df_raw.columns[1:]:\n",
    "        code_map[col] = str(df_raw.loc[code_row_idx, col])\n",
    "    # 删除代码行\n",
    "    df_raw = df_raw.drop(index=code_row_idx).reset_index(drop=True)\n",
    "\n",
    "# 把日期列转为 datetime，并按日期降序/升序均可\n",
    "date_col = df_raw.columns[0]\n",
    "df_raw[date_col] = pd.to_datetime(df_raw[date_col])\n",
    "df_raw = df_raw.sort_values(date_col).reset_index(drop=True)\n",
    "\n",
    "# ===== 2) 计算每只 ETF 的 signed-log 指标 =====\n",
    "flow_cols = df_raw.columns[1:]  # 除日期外的所有 ETF 列\n",
    "\n",
    "# 把数值列转 float\n",
    "for c in flow_cols:\n",
    "    df_raw[c] = pd.to_numeric(df_raw[c], errors=\"coerce\")\n",
    "\n",
    "# 计算尺度 s_i = median(|Flow|)\n",
    "scale = df_raw[flow_cols].abs().median(axis=0, skipna=True).replace(0, np.nan)\n",
    "# 对极端小的尺度做兜底，避免除以 0\n",
    "scale = scale.fillna(1.0)\n",
    "\n",
    "def signed_log_transform(values, s):\n",
    "    # values: pd.Series of flows for one ETF\n",
    "    return np.sign(values) * np.log1p(np.abs(values) / s)\n",
    "\n",
    "# 生成 slog 宽表\n",
    "slog_wide = pd.DataFrame({date_col: df_raw[date_col]})\n",
    "for c in flow_cols:\n",
    "    s_i = scale[c]\n",
    "    slog_wide[c] = signed_log_transform(df_raw[c], s_i)\n",
    "\n",
    "# winsorize slog，默认 5%–95%\n",
    "winsor = True\n",
    "lower_q, upper_q = 0.05, 0.95\n",
    "if winsor:\n",
    "    for c in flow_cols:\n",
    "        lo = slog_wide[c].quantile(lower_q)\n",
    "        hi = slog_wide[c].quantile(upper_q)\n",
    "        slog_wide[c] = slog_wide[c].clip(lower=lo, upper=hi)\n",
    "\n",
    "# ===== 3) 生成“长表”便于后续 merge / 分组分析 =====\n",
    "long = df_raw.melt(id_vars=[date_col], value_vars=flow_cols,\n",
    "                   var_name=\"etf_name\", value_name=\"flow_raw\")\n",
    "long[\"s_i\"] = long[\"etf_name\"].map(scale.to_dict())\n",
    "long[\"y_slog\"] = signed_log_transform(long[\"flow_raw\"], long[\"s_i\"])\n",
    "\n",
    "if winsor:\n",
    "    # 按每只 ETF 分别 winsorize\n",
    "    def _clip_grp(g):\n",
    "        lo = g[\"y_slog\"].quantile(lower_q)\n",
    "        hi = g[\"y_slog\"].quantile(upper_q)\n",
    "        g[\"y_slog\"] = g[\"y_slog\"].clip(lo, hi)\n",
    "        return g\n",
    "    long = long.groupby(\"etf_name\", group_keys=False).apply(_clip_grp)\n",
    "\n",
    "# 若你需要把“基金代码”也带上（来自第二行）\n",
    "if code_map:\n",
    "    long[\"etf_code\"] = long[\"etf_name\"].map(code_map)\n",
    "else:\n",
    "    long[\"etf_code\"] = np.nan\n",
    "\n",
    "# ===== 4) 导出结果 =====\n",
    "# 宽表：每列一只 ETF 的 slog\n",
    "from pathlib import Path\n",
    "\n",
    "# 目标文件夹\n",
    "out_dir = Path(BASE_DIR /\"新数据\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)  # 若不存在则创建\n",
    "\n",
    "# 文件名\n",
    "output_wide = out_dir / \"etf_y_slog_wide.csv\"\n",
    "output_long = out_dir / \"etf_y_slog_long.csv\"\n",
    "\n",
    "# 导出\n",
    "slog_wide.to_csv(output_wide, index=False, encoding=\"utf-8-sig\")\n",
    "long.to_csv(output_long, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"完成：\", output_wide, output_long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2934bc",
   "metadata": {},
   "source": [
    "# D的处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b688944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] MEMORY_DAYS=14, TAIL_AT_MEMORY=0.01, lambda_day=0.3289, decay=0.7197, decay^14=0.0100\n",
      "[read] 汽车（国家）: rows=16, valid_dates=16, date_range=(2025-05-21 00:00:00, 2025-10-14 00:00:00)\n",
      "[read] 汽车（地方）: rows=198, valid_dates=198, date_range=(2025-05-01 00:00:00, 2025-10-30 00:00:00)\n",
      "[read] 家用电器（国家）: rows=22, valid_dates=22, date_range=(2025-05-06 00:00:00, 2025-11-01 00:00:00)\n",
      "[read] 家用电器（地方）: rows=34, valid_dates=34, date_range=(2025-05-01 00:00:00, 2025-10-20 00:00:00)\n",
      "[read] 餐饮（国家）: rows=18, valid_dates=15, date_range=(2025-05-07 00:00:00, 2025-10-13 00:00:00)\n",
      "[read] 餐饮（地方）: rows=81, valid_dates=64, date_range=(2025-05-14 00:00:00, 2025-10-30 00:00:00)\n",
      "[read] 旅游（国家）: rows=36, valid_dates=36, date_range=(2025-05-05 00:00:00, 2025-10-29 00:00:00)\n",
      "[read] 旅游（地方法规）: rows=38, valid_dates=38, date_range=(2025-05-18 00:00:00, 2025-10-25 00:00:00)\n",
      "[read] 动漫游戏（国家）: rows=9, valid_dates=9, date_range=(2025-04-30 00:00:00, 2025-10-31 00:00:00)\n",
      "[read] 动漫游戏（地方）: rows=40, valid_dates=37, date_range=(2025-05-06 00:00:00, 2025-10-20 00:00:00)\n",
      "[read] 文娱（国家）: rows=23, valid_dates=21, date_range=(2025-01-09 00:00:00, 2025-10-28 00:00:00)\n",
      "[read] 文娱（地方）: rows=33, valid_dates=31, date_range=(2025-05-06 00:00:00, 2025-11-03 00:00:00)\n",
      "[read] 汽车（国家）: rows=16, valid_dates=16, date_range=(2025-05-21 00:00:00, 2025-10-14 00:00:00)\n",
      "[read] 汽车（地方）: rows=198, valid_dates=198, date_range=(2025-05-01 00:00:00, 2025-10-30 00:00:00)\n",
      "[read] 家用电器（国家）: rows=22, valid_dates=22, date_range=(2025-05-06 00:00:00, 2025-11-01 00:00:00)\n",
      "[read] 家用电器（地方）: rows=34, valid_dates=34, date_range=(2025-05-01 00:00:00, 2025-10-20 00:00:00)\n",
      "[read] 餐饮（国家）: rows=18, valid_dates=15, date_range=(2025-05-07 00:00:00, 2025-10-13 00:00:00)\n",
      "[read] 餐饮（地方）: rows=81, valid_dates=64, date_range=(2025-05-14 00:00:00, 2025-10-30 00:00:00)\n",
      "[read] 旅游（国家）: rows=36, valid_dates=36, date_range=(2025-05-05 00:00:00, 2025-10-29 00:00:00)\n",
      "[read] 旅游（地方法规）: rows=38, valid_dates=38, date_range=(2025-05-18 00:00:00, 2025-10-25 00:00:00)\n",
      "[read] 动漫游戏（国家）: rows=9, valid_dates=9, date_range=(2025-04-30 00:00:00, 2025-10-31 00:00:00)\n",
      "[read] 动漫游戏（地方）: rows=40, valid_dates=37, date_range=(2025-05-06 00:00:00, 2025-10-20 00:00:00)\n",
      "[read] 文娱（国家）: rows=23, valid_dates=21, date_range=(2025-01-09 00:00:00, 2025-10-28 00:00:00)\n",
      "[read] 文娱（地方）: rows=33, valid_dates=31, date_range=(2025-05-06 00:00:00, 2025-11-03 00:00:00)\n",
      "[OK] 全部完成：\n",
      "- 总长表: C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\policy_D_daily__ALL.csv\n",
      "- 总宽表: C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\policy_D_daily__wide.csv\n",
      "- 分行业: \n",
      "    C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\policy_D_daily__汽车.csv\n",
      "    C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\policy_D_daily__家用电器.csv\n",
      "    C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\policy_D_daily__餐饮.csv\n",
      "    C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\policy_D_daily__旅游.csv\n",
      "    C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\policy_D_daily__动漫游戏.csv\n",
      "    C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\policy_D_daily__文娱.csv\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas numpy openpyxl\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ================= 用户配置 =================\n",
    "XLSX_PATH = BASE_DIR / \"原始数据\" / \"政策特征分析结果.xlsx\"\n",
    "\n",
    "# 输出目录（建议统一放到一个可追踪的相对目录下）\n",
    "OUT_DIR = BASE_DIR / \"新数据\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_ALL  = OUT_DIR / \"policy_D_daily__ALL.csv\"      # 合并后的总表\n",
    "OUT_WIDE = OUT_DIR / \"policy_D_daily__wide.csv\"     # 宽表（D_nat, D_loc, D_total）\n",
    "\n",
    "COL_DATE   = \"公布日期\"\n",
    "COL_FACTS  = [\"层级\", \"新颖度\", \"财政性\", \"约束力度\", \"覆盖范围\"]\n",
    "\n",
    "NORM_WINDOW_DAYS = 30\n",
    "EPS_DENOM = 1\n",
    "\n",
    "WINDOW_START = pd.Timestamp(\"2025-05-15\")\n",
    "WINDOW_END   = pd.Timestamp(\"2025-10-31\")\n",
    "\n",
    "MEMORY_DAYS = 14\n",
    "TAIL_AT_MEMORY = 0.01\n",
    "\n",
    "lambda_day = -np.log(TAIL_AT_MEMORY) / MEMORY_DAYS\n",
    "decay = float(np.exp(-lambda_day))\n",
    "print(f\"[info] MEMORY_DAYS={MEMORY_DAYS}, TAIL_AT_MEMORY={TAIL_AT_MEMORY}, \"\n",
    "      f\"lambda_day={lambda_day:.4f}, decay={decay:.4f}, decay^{MEMORY_DAYS}={decay**MEMORY_DAYS:.4f}\")\n",
    "\n",
    "SHEETS_BY_INDUSTRY = {\n",
    "    \"汽车\": {\"国家\": \"汽车（国家）\", \"地方\": \"汽车（地方）\"},\n",
    "    \"家用电器\": {\"国家\": \"家用电器（国家）\", \"地方\": \"家用电器（地方）\"},\n",
    "    # 注意：这里你原来国家/地方写成同一个 sheet 名，建议你确认后修改\n",
    "    \"餐饮\": {\"国家\": \"餐饮（国家）\", \"地方\": \"餐饮（地方）\"},\n",
    "    \"旅游\": {\"国家\": \"旅游（国家）\", \"地方\": \"旅游（地方法规）\"},\n",
    "    \"动漫游戏\": {\"国家\": \"动漫游戏（国家）\", \"地方\": \"动漫游戏（地方）\"},\n",
    "    \"文娱\": {\"国家\": \"文娱（国家）\", \"地方\": \"文娱（地方）\"},\n",
    "}\n",
    "\n",
    "EXTRA_DAYS_AFTER_LAST = MEMORY_DAYS\n",
    "\n",
    "\n",
    "# ============== 工具函数 ==============\n",
    "def _read_policies_from_sheet(xlsx_path, sheet_name):\n",
    "    \"\"\"从单个 sheet 读取（date, weight）两列，鲁棒日期/数值解析。\"\"\"\n",
    "    df = pd.read_excel(xlsx_path, sheet_name=sheet_name)\n",
    "    # 清理列名里的空格\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    need = [COL_DATE] + COL_FACTS\n",
    "    miss = [c for c in need if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"Sheet[{sheet_name}] 缺少列: {miss}\")\n",
    "\n",
    "    tmp = df[need].copy()\n",
    "\n",
    "    # ---- 日期清洗：把非数字全去掉，再按 %Y%m%d 解析；若本来就是 datetime 也能过\n",
    "    if not np.issubdtype(tmp[COL_DATE].dtype, np.datetime64):\n",
    "        # 统一成字符串\n",
    "        tmp[COL_DATE] = tmp[COL_DATE].astype(str)\n",
    "        # 去掉所有非数字字符，例如 '2024. 05. 21' -> '20240521'\n",
    "        tmp[COL_DATE] = tmp[COL_DATE].str.replace(r\"\\D\", \"\", regex=True)\n",
    "        # 过滤长度不对的\n",
    "        tmp = tmp[tmp[COL_DATE].str.len() >= 8]\n",
    "        tmp[COL_DATE] = pd.to_datetime(tmp[COL_DATE].str[:8], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "    tmp = tmp.dropna(subset=[COL_DATE])\n",
    "\n",
    "    # ---- 权重列：转成数值，无法解析的按 1.0 处理（中性权重）\n",
    "    for c in COL_FACTS:\n",
    "        tmp[c] = pd.to_numeric(tmp[c], errors=\"coerce\")\n",
    "        tmp[c] = tmp[c].fillna(1.0)\n",
    "\n",
    "    tmp[\"w\"] = tmp[COL_FACTS].prod(axis=1)\n",
    "\n",
    "    # 打印一下读取统计，便于检查\n",
    "    print(f\"[read] {sheet_name}: rows={len(df)}, valid_dates={tmp.shape[0]}, \"\n",
    "          f\"date_range=({tmp[COL_DATE].min()}, {tmp[COL_DATE].max()})\")\n",
    "\n",
    "    return tmp.rename(columns={COL_DATE: \"date\"})[[\"date\", \"w\"]]\n",
    "\n",
    "\n",
    "def _decay_daily_series(policy_df, start_date, end_date, decay, memory_days):\n",
    "    \"\"\"\n",
    "    生成“有限记忆”的指数衰减库存序列：\n",
    "      D_t = decay * D_{t-1} + arrivals_t - decay**memory_days * arrivals_{t-memory_days}\n",
    "\n",
    "    这样 D_t 只依赖最近 memory_days 天的 arrivals。\n",
    "    返回:\n",
    "      D: 指数衰减库存序列\n",
    "      arrivals: 每日新到达的权重和（用于标准化）\n",
    "    \"\"\"\n",
    "    idx = pd.date_range(start_date, end_date, freq=\"D\")\n",
    "    arrivals = pd.Series(0.0, index=idx, name=\"arrivals\")\n",
    "    if not policy_df.empty:\n",
    "        arrivals = policy_df.groupby(\"date\")[\"w\"].sum().reindex(idx, fill_value=0.0)\n",
    "        arrivals.name = \"arrivals\"\n",
    "\n",
    "    arrivals_vals = arrivals.values\n",
    "    D_vals = np.zeros(len(idx), dtype=float)\n",
    "    decay_k = decay ** memory_days  # decay 的 memory_days 次方\n",
    "\n",
    "    for t in range(len(idx)):\n",
    "        prev = D_vals[t-1] if t > 0 else 0.0\n",
    "        D_t = decay * prev + arrivals_vals[t]\n",
    "        # 超过 memory_days 的那一日从库存中剔除\n",
    "        if t >= memory_days:\n",
    "            D_t -= decay_k * arrivals_vals[t - memory_days]\n",
    "        D_vals[t] = D_t\n",
    "\n",
    "    D = pd.Series(D_vals, index=idx, name=\"D\")\n",
    "    return D, arrivals\n",
    "\n",
    "\n",
    "# ============== 主流程 ==============\n",
    "all_rows_long = []   # 长表：industry, tier, date, D\n",
    "date_min, date_max = None, None\n",
    "\n",
    "# 先扫描所有 sheet，拿到全局起止日期\n",
    "for ind, tiers in SHEETS_BY_INDUSTRY.items():\n",
    "    for tier, sh in tiers.items():\n",
    "        dfj = _read_policies_from_sheet(XLSX_PATH, sh)\n",
    "        if not dfj.empty:\n",
    "            dmin, dmax = dfj[\"date\"].min(), dfj[\"date\"].max()\n",
    "            date_min = dmin if date_min is None else min(date_min, dmin)\n",
    "            date_max = dmax if date_max is None else max(date_max, dmax)\n",
    "\n",
    "if date_min is None:\n",
    "    raise RuntimeError(\"没有读到任何政策日期，请检查 SHEETS_BY_INDUSTRY 和列名。\")\n",
    "\n",
    "start_date = date_min\n",
    "end_date   = date_max + pd.Timedelta(days=EXTRA_DAYS_AFTER_LAST)\n",
    "\n",
    "# 分行业逐个生成（国家/地方/合计）\n",
    "per_industry_files = []\n",
    "for ind, tiers in SHEETS_BY_INDUSTRY.items():\n",
    "    # 读国家/地方两张表（允许缺一张）\n",
    "    nat_df = _read_policies_from_sheet(XLSX_PATH, tiers.get(\"国家\")) if tiers.get(\"国家\") else pd.DataFrame(columns=[\"date\",\"w\"])\n",
    "    loc_df = _read_policies_from_sheet(XLSX_PATH, tiers.get(\"地方\")) if tiers.get(\"地方\") else pd.DataFrame(columns=[\"date\",\"w\"])\n",
    "\n",
    "    # 生成 D 与 arrivals（有限记忆版本）\n",
    "    D_nat, arr_nat = _decay_daily_series(nat_df, start_date, end_date, decay=decay, memory_days=MEMORY_DAYS)\n",
    "    D_loc, arr_loc = _decay_daily_series(loc_df, start_date, end_date, decay=decay, memory_days=MEMORY_DAYS)\n",
    "\n",
    "    D_total = D_nat.add(D_loc, fill_value=0.0)\n",
    "    arr_total = arr_nat.add(arr_loc, fill_value=0.0)\n",
    "\n",
    "    # --- 防单增标准化：用最近 W 天到达量做归一 ---\n",
    "    den_nat   = arr_nat.rolling(NORM_WINDOW_DAYS, min_periods=1).sum()\n",
    "    den_loc   = arr_loc.rolling(NORM_WINDOW_DAYS, min_periods=1).sum()\n",
    "    den_total = arr_total.rolling(NORM_WINDOW_DAYS, min_periods=1).sum()\n",
    "\n",
    "    D_nat_norm   = D_nat   / (den_nat   + EPS_DENOM)\n",
    "    D_loc_norm   = D_loc   / (den_loc   + EPS_DENOM)\n",
    "    D_total_norm = D_total / (den_total + EPS_DENOM)\n",
    "\n",
    "    # 宽表（该行业）\n",
    "    df_wide = pd.DataFrame({\n",
    "        \"industry\": ind,\n",
    "        \"date\": D_total.index,\n",
    "        \"D_nat\": D_nat.values,\n",
    "        \"D_loc\": D_loc.values,\n",
    "        \"D_total\": D_total.values,\n",
    "        \"D_nat_norm\": D_nat_norm.values,\n",
    "        \"D_loc_norm\": D_loc_norm.values,\n",
    "        \"D_total_norm\": D_total_norm.values\n",
    "    })\n",
    "\n",
    "    # 仅保留指定窗口（如 5/15~10/31）\n",
    "    df_wide = df_wide[(df_wide[\"date\"] >= WINDOW_START) & (df_wide[\"date\"] <= WINDOW_END)].reset_index(drop=True)\n",
    "\n",
    "    # 单行业文件\n",
    "    out_ind = OUT_DIR / f\"policy_D_daily__{ind}.csv\"\n",
    "    df_wide.to_csv(out_ind, index=False, encoding=\"utf-8-sig\")\n",
    "    per_industry_files.append(out_ind)\n",
    "\n",
    "    # 长表（累积）——也把归一化列带上\n",
    "    all_rows_long.append(\n",
    "        df_wide.melt(\n",
    "            id_vars=[\"industry\", \"date\"],\n",
    "            value_vars=[\"D_nat\", \"D_loc\", \"D_total\", \"D_nat_norm\", \"D_loc_norm\", \"D_total_norm\"],\n",
    "            var_name=\"which\", value_name=\"D\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 合并输出\n",
    "D_long = pd.concat(all_rows_long, ignore_index=True)\n",
    "D_long = D_long[(D_long[\"date\"] >= WINDOW_START) & (D_long[\"date\"] <= WINDOW_END)].reset_index(drop=True)\n",
    "D_long.to_csv(OUT_ALL, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "D_wide_all = D_long.pivot_table(index=[\"industry\", \"date\"], columns=\"which\", values=\"D\", aggfunc=\"sum\").reset_index()\n",
    "# 确保三列齐全\n",
    "for c in [\"D_nat\", \"D_loc\", \"D_total\"]:\n",
    "    if c not in D_wide_all.columns:\n",
    "        D_wide_all[c] = 0.0\n",
    "D_wide_all = D_wide_all.sort_values([\"industry\", \"date\"])\n",
    "D_wide_all.to_csv(OUT_WIDE, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[OK] 全部完成：\\n- 总长表: {OUT_ALL}\\n- 总宽表: {OUT_WIDE}\\n- 分行业: \")\n",
    "for p in per_industry_files:\n",
    "    print(\"   \", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0ed19",
   "metadata": {},
   "source": [
    "# 处理OI的\n",
    "因为有六个行业需要分别处理，定义处理原始数据的函数，后续直接调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47fd2994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy openpyxl\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Sequence, Optional, Tuple, Union\n",
    "\n",
    "# =========================\n",
    "# 小工具（和你原逻辑一致）\n",
    "# =========================\n",
    "\n",
    "DATE_COL_CAND   = [\"时间\", \"date\", \"日期\"]\n",
    "REGION_COL_CAND = [\"省份/市\", \"地区\", \"region\"]\n",
    "KW_COL_CAND     = [\"关键词\", \"keyword\"]\n",
    "TOTAL_COL_CAND  = [\"搜索pc+移动\", \"搜索pc＋移动\", \"总搜索\", \"总量\"]\n",
    "PC_COL_CAND     = [\"搜索pc\", \"pc\"]\n",
    "MOBILE_COL_CAND = [\"搜索移动\", \"移动\", \"mobile\"]\n",
    "\n",
    "def read_any(path_stem: Path) -> Tuple[pd.DataFrame, Path]:\n",
    "    \"\"\"path_stem 为不带后缀的 Path，自动尝试 .xlsx/.csv\"\"\"\n",
    "    for ext in [\".xlsx\", \".csv\"]:\n",
    "        path = path_stem.with_suffix(ext)\n",
    "        if path.exists():\n",
    "            if ext == \".xlsx\":\n",
    "                return pd.read_excel(path, sheet_name=0), path\n",
    "            else:\n",
    "                for enc in [\"utf-8-sig\", \"utf-8\", \"gbk\"]:\n",
    "                    try:\n",
    "                        df = pd.read_csv(path, sep=None, engine=\"python\", encoding=enc)\n",
    "                        return df, path\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    raise FileNotFoundError(f\"找不到文件：{path_stem}(.xlsx/.csv)\")\n",
    "\n",
    "def pick_col(df: pd.DataFrame, cands: Sequence[str], default: Optional[str] = None) -> Optional[str]:\n",
    "    cols = [c for c in cands if c in df.columns]\n",
    "    return cols[0] if cols else default\n",
    "\n",
    "def clean_numeric(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(\n",
    "        s.astype(str).str.replace(\",\", \"\", regex=False)\n",
    "         .str.replace(\" \", \"\", regex=False)\n",
    "         .str.replace(\"（\", \"(\", regex=False).str.replace(\"）\", \")\", regex=False)\n",
    "         .str.replace(r\"[^\\d\\.\\-]\", \"\", regex=True),\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "def winsorize_series(s: pd.Series, lo: float = 0.05, hi: float = 0.99) -> pd.Series:\n",
    "    ql, qh = s.quantile(lo), s.quantile(hi)\n",
    "    return s.clip(lower=ql, upper=qh)\n",
    "\n",
    "def _filter_national_rows(df: pd.DataFrame, region_col: Optional[str]) -> pd.DataFrame:\n",
    "    \"\"\"若有地区列，只保留 全国/中国/全部 或缺失。\"\"\"\n",
    "    if region_col is None:\n",
    "        return df\n",
    "    reg = df[region_col].astype(str).str.strip()\n",
    "    keep = reg.isin([\"全国\", \"中国\", \"全部\"]) | df[region_col].isna()\n",
    "    return df.loc[keep].copy()\n",
    "\n",
    "# =========================\n",
    "# 主函数：构造并导出 OI\n",
    "# =========================\n",
    "\n",
    "def build_oi_from_baidu_files(\n",
    "    p_raw: Union[str, Path],\n",
    "    p_out: Union[str, Path],\n",
    "    industry_tag: str,\n",
    "    file_stems: Sequence[str],\n",
    "    *,\n",
    "    date_col_cand: Sequence[str] = DATE_COL_CAND,\n",
    "    region_col_cand: Sequence[str] = REGION_COL_CAND,\n",
    "    kw_col_cand: Sequence[str] = KW_COL_CAND,\n",
    "    total_col_cand: Sequence[str] = TOTAL_COL_CAND,\n",
    "    pc_col_cand: Sequence[str] = PC_COL_CAND,\n",
    "    mobile_col_cand: Sequence[str] = MOBILE_COL_CAND,\n",
    "    winsor_lo: float = 0.05,\n",
    "    winsor_hi: float = 0.99,\n",
    "    log1p: bool = True,\n",
    "    export_wide: bool = True,\n",
    "    export_simple: bool = True,\n",
    ") -> Tuple[pd.DataFrame, Optional[Path], Optional[Path]]:\n",
    "    \"\"\"\n",
    "    读取同一行业的多个关键词文件（xlsx/csv），构造日度 OI 并导出。\n",
    "\n",
    "    参数\n",
    "    - p_raw: 原始关键词文件所在文件夹（如 .../原始数据/指数/文娱传媒）\n",
    "    - p_out: 输出文件夹（如 .../新数据）\n",
    "    - industry_tag: 行业名，用于输出文件名\n",
    "    - file_stems: 文件名列表（不带后缀），可为 6 个或任意个\n",
    "\n",
    "    返回\n",
    "    - Z: 宽表（index=date；列含各关键词 z_*, OI_mean, K_used）\n",
    "    - out_wide_path / out_simple_path: 导出路径（若对应 export_* 为 False 则为 None）\n",
    "    \"\"\"\n",
    "    p_raw = Path(p_raw)\n",
    "    p_out = Path(p_out)\n",
    "    p_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dfs = []\n",
    "    for stem in file_stems:\n",
    "        df, real_path = read_any(p_raw / stem)\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "        date_col   = pick_col(df, date_col_cand, df.columns[0])\n",
    "        region_col = pick_col(df, region_col_cand, None)\n",
    "        kw_col     = pick_col(df, kw_col_cand, None)\n",
    "\n",
    "        total_col  = pick_col(df, total_col_cand, None)\n",
    "        pc_col     = pick_col(df, pc_col_cand, None)\n",
    "        mob_col    = pick_col(df, mobile_col_cand, None)\n",
    "\n",
    "        use = df.copy()\n",
    "        use = _filter_national_rows(use, region_col)\n",
    "\n",
    "        use[\"date\"] = pd.to_datetime(use[date_col], errors=\"coerce\")\n",
    "        use = use.dropna(subset=[\"date\"])\n",
    "\n",
    "        # 构造 BI\n",
    "        if total_col and total_col in use.columns:\n",
    "            bi = clean_numeric(use[total_col])\n",
    "        else:\n",
    "            if pc_col is None or mob_col is None:\n",
    "                raise ValueError(f\"{real_path} 缺少总量列，且 pc/移动列不全。\")\n",
    "            bi = clean_numeric(use[pc_col]) + clean_numeric(use[mob_col])\n",
    "\n",
    "        # 关键词名（优先用文件内关键词列，否则用 stem）\n",
    "        if kw_col and kw_col in use.columns:\n",
    "            kw_name = str(use[kw_col].dropna().iloc[0]) if not use[kw_col].dropna().empty else str(stem)\n",
    "        else:\n",
    "            kw_name = str(stem)\n",
    "\n",
    "        dfs.append(pd.DataFrame({\"date\": use[\"date\"], \"keyword\": kw_name, \"BI\": bi}))\n",
    "\n",
    "    raw_all = pd.concat(dfs, ignore_index=True).sort_values(\"date\")\n",
    "\n",
    "    # 每个关键词：winsorize + (log1p) + z-score\n",
    "    zs = []\n",
    "    for kw, g in raw_all.groupby(\"keyword\"):\n",
    "        s = g.set_index(\"date\")[\"BI\"].sort_index()\n",
    "        s = winsorize_series(s, winsor_lo, winsor_hi)\n",
    "\n",
    "        x = np.log1p(s) if log1p else s\n",
    "        mu, sd = x.mean(), x.std(ddof=1)\n",
    "        if not np.isfinite(sd) or sd == 0:\n",
    "            sd = 1.0\n",
    "\n",
    "        z = ((x - mu) / sd).rename(f\"z_{kw}\")\n",
    "        zs.append(z)\n",
    "\n",
    "    Z = pd.concat(zs, axis=1).sort_index()\n",
    "    Z[\"OI_mean\"] = Z.mean(axis=1, skipna=True)\n",
    "    Z[\"K_used\"]  = Z.notna().sum(axis=1)\n",
    "\n",
    "    out_wide_path = None\n",
    "    out_simple_path = None\n",
    "\n",
    "    if export_wide:\n",
    "        out_wide_path = p_out / f\"OI_daily_{industry_tag}.csv\"\n",
    "        Z.reset_index().rename(columns={\"index\": \"date\"}).to_csv(out_wide_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    if export_simple:\n",
    "        out_simple_path = p_out / f\"OI_daily_{industry_tag}_simple.csv\"\n",
    "        Z[[\"OI_mean\"]].reset_index().to_csv(out_simple_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    return Z, out_wide_path, out_simple_path\n",
    "\n",
    "# =========================\n",
    "# 便捷封装：按你的 p_base 结构调用\n",
    "# =========================\n",
    "\n",
    "def build_oi_one_industry(\n",
    "    p_base: Union[str, Path],\n",
    "    industry_tag: str,\n",
    "    file_stems: Sequence[str],\n",
    "    *,\n",
    "    raw_subdir: Sequence[str] = (\"原始数据\", \"指数\"),\n",
    "    out_subdir: Sequence[str] = (\"新数据\",),\n",
    ") -> Tuple[pd.DataFrame, Optional[Path], Optional[Path]]:\n",
    "    \"\"\"\n",
    "    假设你的目录结构：\n",
    "    p_base/原始数据/指数/<industry_tag>/[1.xlsx|1.csv ...]\n",
    "    p_base/新数据/...\n",
    "    \"\"\"\n",
    "    p_base = Path(p_base)\n",
    "    p_raw = p_base.joinpath(*raw_subdir, industry_tag)\n",
    "    p_out = p_base.joinpath(*out_subdir)\n",
    "    return build_oi_from_baidu_files(p_raw=p_raw, p_out=p_out, industry_tag=industry_tag, file_stems=file_stems)\n",
    "\n",
    "def build_oi_batch(\n",
    "    p_base: Union[str, Path],\n",
    "    jobs: Sequence[Tuple[str, Sequence[str]]],\n",
    ") -> List[Tuple[str, Optional[Path], Optional[Path]]]:\n",
    "    \"\"\"\n",
    "    批量跑多个行业或多组文件名。\n",
    "    jobs: [(industry_tag, [file1, file2, ...]), ...]\n",
    "    返回每个行业的导出路径。\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for industry_tag, stems in jobs:\n",
    "        _, out_wide, out_simple = build_oi_one_industry(p_base, industry_tag, stems)\n",
    "        results.append((industry_tag, out_wide, out_simple))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eeb57f",
   "metadata": {},
   "source": [
    "调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d0cbb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文娱传媒 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_文娱传媒.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_文娱传媒_simple.csv\n",
      "动漫游戏 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_动漫游戏.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_动漫游戏_simple.csv\n",
      "家用电器 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_家用电器.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_家用电器_simple.csv\n",
      "旅游 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_旅游.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_旅游_simple.csv\n",
      "新能源汽车 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_新能源汽车.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_新能源汽车_simple.csv\n",
      "食品 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_食品.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\OI_daily_食品_simple.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "jobs = [\n",
    "    (\"文娱传媒\", [\"1\",\"2\",\"3\"]),\n",
    "    (\"动漫游戏\", [\"1\",\"2\",\"3\"]),\n",
    "    (\"家用电器\",     [\"1\",\"2\",\"3\"]),\n",
    "    (\"旅游\",     [\"1\",\"2\",\"3\"]),\n",
    "    (\"新能源汽车\",     [\"1\",\"2\",\"3\"]),\n",
    "    (\"食品\",     [\"1\",\"2\",\"3\"]),\n",
    "]\n",
    "\n",
    "res = build_oi_batch(BASE_DIR, jobs)\n",
    "for industry, wide_path, simple_path in res:\n",
    "    print(industry, wide_path, simple_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac31e9a",
   "metadata": {},
   "source": [
    "# 处理ET的\n",
    "\n",
    "同样也是需要六个行业分别处理，也是先写函数再调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef29c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy chardet\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Optional, Tuple, Union, List\n",
    "\n",
    "# =========================\n",
    "# 默认候选列名（可按需覆盖）\n",
    "# =========================\n",
    "DATE_COL_CAND_DEFAULT  = [\"update\", \"日期\", \"date\", \"时间\"]\n",
    "SCORE_COL_CAND_DEFAULT = [\"sentiment_score\", \"sentiment\", \"score\", \"情感分\"]\n",
    "\n",
    "# =========================\n",
    "# 小工具\n",
    "# =========================\n",
    "def read_csv_auto(path: Union[str, Path],\n",
    "                  encodings: Sequence[str] = (\"utf-8-sig\", \"utf-8\", \"gbk\")) -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"自动尝试编码 + 自动分隔符\"\"\"\n",
    "    path = str(path)\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=None, engine=\"python\", encoding=enc)\n",
    "            return df, enc\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"读入CSV失败：请确认路径/编码/分隔符。最后错误：{last_err}\")\n",
    "\n",
    "def pick_col(df: pd.DataFrame, cands: Sequence[str]) -> Optional[str]:\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def parse_date_any(s, default_year: int = 2025) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    多格式日期解析：\n",
    "    1) 2025-10-31 / 2025/10/31 / 2025.10.31 / 2025年10月31日\n",
    "    2) 10月31日 / 10/31 / 10-31（补 default_year）\n",
    "    3) pandas 兜底\n",
    "    \"\"\"\n",
    "    x = str(s).strip()\n",
    "    if not x:\n",
    "        return pd.NaT\n",
    "\n",
    "    m = re.match(r\"^\\s*(\\d{4})[年\\-/\\.](\\d{1,2})[月\\-/\\.](\\d{1,2})[日]?\\s*$\", x)\n",
    "    if m:\n",
    "        y, mo, d = map(int, m.groups())\n",
    "        return pd.Timestamp(year=y, month=mo, day=d)\n",
    "\n",
    "    m = re.match(r\"^\\s*(\\d{1,2})\\s*[月/\\-\\.]\\s*(\\d{1,2})\\s*[日]?\\s*$\", x)\n",
    "    if m:\n",
    "        mo, d = map(int, m.groups())\n",
    "        return pd.Timestamp(year=default_year, month=mo, day=d)\n",
    "\n",
    "    try:\n",
    "        return pd.to_datetime(x, errors=\"raise\")\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "# =========================\n",
    "# 主函数：从“单个行业的情绪CSV”构造 ET 日度序列并导出\n",
    "# =========================\n",
    "def build_et_from_sentiment_csv(\n",
    "    input_path: Union[str, Path],\n",
    "    out_dir: Union[str, Path],\n",
    "    industry_tag: str,\n",
    "    *,\n",
    "    date_col_cand: Sequence[str] = DATE_COL_CAND_DEFAULT,\n",
    "    score_col_cand: Sequence[str] = SCORE_COL_CAND_DEFAULT,\n",
    "    default_year: int = 2025,\n",
    "    ema_halflife: float = 7,\n",
    "    export: bool = True,\n",
    "    out_filename: Optional[str] = None,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[pd.DataFrame, Optional[Path]]:\n",
    "    \"\"\"\n",
    "    读取行业情绪明细CSV（含日期列 + sentiment score列），聚合为日度：\n",
    "      - ET_mean: 日均值\n",
    "      - ET_var : 日方差\n",
    "      - ET_n   : 样本数\n",
    "      - ET_z   : 对 ET_mean 做 z-score\n",
    "      - ET_ema_hl{halflife}: 对 ET_z 做 EWM(halflife=7) 平滑\n",
    "    并导出到 out_dir/ET_daily_{industry}.csv\n",
    "\n",
    "    返回：\n",
    "      daily_df, out_path(若 export=False 则 None)\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df, encoding_used = read_csv_auto(input_path)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    DATE_COL = pick_col(df, date_col_cand)\n",
    "    SCORE_COL = pick_col(df, score_col_cand)\n",
    "    if DATE_COL is None:\n",
    "        raise ValueError(f\"找不到日期列（候选：{list(date_col_cand)}），现有列：{df.columns.tolist()}\")\n",
    "    if SCORE_COL is None:\n",
    "        raise ValueError(f\"找不到情感分列（候选：{list(score_col_cand)}），现有列：{df.columns.tolist()}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\">>> 读入成功：{input_path} (encoding={encoding_used})\")\n",
    "        print(\">>> 列名：\", df.columns.tolist())\n",
    "        print(\">>> 日期列/情感列：\", DATE_COL, \"/\", SCORE_COL)\n",
    "        print(\">>> 日期列前10个原始值：\", df[DATE_COL].astype(str).head(10).tolist())\n",
    "\n",
    "    # 日期解析\n",
    "    df[\"date\"] = df[DATE_COL].apply(lambda x: parse_date_any(x, default_year=default_year))\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"date\"]).copy()\n",
    "    after = len(df)\n",
    "    if verbose:\n",
    "        print(f\">>> 日期可解析行数：{after}/{before}\")\n",
    "\n",
    "    # 情感分转数值\n",
    "    df[SCORE_COL] = pd.to_numeric(df[SCORE_COL], errors=\"coerce\")\n",
    "    non_na_before = len(df)\n",
    "    df = df.dropna(subset=[SCORE_COL]).copy()\n",
    "    if verbose:\n",
    "        print(f\">>> 情感分非缺失行数：{len(df)}/{non_na_before}（丢弃 {non_na_before - len(df)} 行非数值）\")\n",
    "\n",
    "    # 日度聚合\n",
    "    daily = (\n",
    "        df.groupby(\"date\")[SCORE_COL]\n",
    "          .agg(ET_mean=\"mean\", ET_var=\"var\", ET_n=\"size\")\n",
    "          .reset_index()\n",
    "          .sort_values(\"date\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    if len(daily) == 0:\n",
    "        raise RuntimeError(\"聚合后为空：多半是日期或情感分列没识别到；请检查列名候选与原始数据。\")\n",
    "\n",
    "    # 标准化 + EMA(半衰期)\n",
    "    mu = daily[\"ET_mean\"].mean()\n",
    "    sd = daily[\"ET_mean\"].std(ddof=1)\n",
    "    if not np.isfinite(sd) or sd == 0:\n",
    "        sd = 1.0\n",
    "\n",
    "    daily[\"ET_z\"] = (daily[\"ET_mean\"] - mu) / sd\n",
    "    daily[f\"ET_ema_hl{ema_halflife:g}\"] = daily[\"ET_z\"].ewm(halflife=ema_halflife, min_periods=1).mean()\n",
    "\n",
    "    out_path = None\n",
    "    if export:\n",
    "        if out_filename is None:\n",
    "            out_filename = f\"ET_daily_{industry_tag}.csv\"\n",
    "        out_path = out_dir / out_filename\n",
    "        daily.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        if verbose:\n",
    "            print(\">>> 已导出：\", out_path)\n",
    "            print(daily.head(10))\n",
    "\n",
    "    return daily, out_path\n",
    "\n",
    "# =========================\n",
    "# 便捷封装：按你的 p_base 结构调用（和 OI 一致）\n",
    "# =========================\n",
    "def build_et_one_industry(\n",
    "    p_base: Union[str, Path],\n",
    "    industry_tag: str,\n",
    "    input_filename: str,\n",
    "    *,\n",
    "    raw_subdir: Sequence[str] = (\"原始数据\",),\n",
    "    out_subdir: Sequence[str] = (\"新数据\",),\n",
    "    **kwargs\n",
    ") -> Tuple[pd.DataFrame, Optional[Path]]:\n",
    "    \"\"\"\n",
    "    假设情绪明细在：\n",
    "      p_base/原始数据/<input_filename>\n",
    "    输出到：\n",
    "      p_base/新数据/ET_daily_{industry}.csv\n",
    "    \"\"\"\n",
    "    p_base = Path(p_base)\n",
    "    input_path = p_base.joinpath(*raw_subdir, input_filename)\n",
    "    out_dir = p_base.joinpath(*out_subdir)\n",
    "    return build_et_from_sentiment_csv(\n",
    "        input_path=input_path,\n",
    "        out_dir=out_dir,\n",
    "        industry_tag=industry_tag,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "def build_et_batch(\n",
    "    p_base: Union[str, Path],\n",
    "    jobs: Sequence[Tuple[str, str]],\n",
    "    *,\n",
    "    raw_subdir: Sequence[str] = (\"原始数据\",),\n",
    "    out_subdir: Sequence[str] = (\"新数据\",),\n",
    "    **kwargs\n",
    ") -> List[Tuple[str, Optional[Path]]]:\n",
    "    \"\"\"\n",
    "    批量跑多个行业：\n",
    "      jobs = [(industry_tag, input_filename), ...]\n",
    "    返回每个行业导出路径\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for industry_tag, input_filename in jobs:\n",
    "        _, out_path = build_et_one_industry(\n",
    "            p_base=p_base,\n",
    "            industry_tag=industry_tag,\n",
    "            input_filename=input_filename,\n",
    "            raw_subdir=raw_subdir,\n",
    "            out_subdir=out_subdir,\n",
    "            **kwargs\n",
    "        )\n",
    "        results.append((industry_tag, out_path))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672317e4",
   "metadata": {},
   "source": [
    "调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "869255c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文娱传媒 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\ET_daily_文娱传媒.csv\n",
      "动漫 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\ET_daily_动漫.csv\n",
      "家电 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\ET_daily_家电.csv\n",
      "旅游 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\ET_daily_旅游.csv\n",
      "汽车 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\ET_daily_汽车.csv\n",
      "食品饮料 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\ET_daily_食品饮料.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "jobs = [\n",
    "    (\"文娱传媒\", \"merged_文娱传媒_top3_per_day_sentiment.csv\"),\n",
    "    (\"动漫\", \"merged_动漫_top3_per_day_sentiment.csv\"),\n",
    "    (\"家电\",     \"merged_家电_top3_per_day_sentiment.csv\"),\n",
    "    (\"旅游\",     \"merged_旅游_top3_per_day_sentiment.csv\"),\n",
    "    (\"汽车\",     \"merged_汽车_top3_per_day_sentiment.csv\"),\n",
    "    (\"食品饮料\",     \"merged_食品饮料_top3_per_day_sentiment.csv\"),\n",
    "]\n",
    "\n",
    "res = build_et_batch(\n",
    "    p_base=BASE_DIR,\n",
    "    jobs=jobs,\n",
    "    default_year=2025,\n",
    "    ema_halflife=7,\n",
    "    verbose=False   # 批量时建议关掉打印\n",
    ")\n",
    "\n",
    "for industry, path in res:\n",
    "    print(industry, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d9546",
   "metadata": {},
   "source": [
    "# 处理宏观控制变量的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6908c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已导出：\n",
      "- C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\controls_filled_daily.csv\n",
      "- C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\新数据\\controls_filled_daily_zscore.csv\n"
     ]
    }
   ],
   "source": [
    "# pip install pandas numpy openpyxl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= 路径设置 =========\n",
    "input_path = BASE_DIR / \"原始数据\" / \"宏观控制变量.xlsx\"\n",
    "out_dir    = BASE_DIR / \"新数据\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========= 读入 & 解析表头 =========\n",
    "df0 = pd.read_excel(input_path, header=None)\n",
    "\n",
    "# 第1~4行是表头信息\n",
    "names_row   = df0.iloc[0].astype(str).str.strip().tolist()\n",
    "freq_row    = df0.iloc[1].astype(str).str.strip().tolist()\n",
    "unit_row    = df0.iloc[2].astype(str).str.strip().tolist()\n",
    "# 第1列是“指标名称/频率/单位/指标ID”，后面列是各指标数据\n",
    "data = df0.iloc[4:].copy()  # 从第5行开始是数据\n",
    "\n",
    "# 设定列名：第一列叫 'date_raw'，其他列用“指标名称行”的名字\n",
    "colnames = [\"date_raw\"] + names_row[1:]\n",
    "data.columns = colnames\n",
    "\n",
    "# ========= 日期解析（YYYYMMDD -> datetime） =========\n",
    "data[\"date\"] = pd.to_datetime(data[\"date_raw\"].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n",
    "data = data.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# ========= 数值化：去掉千分位逗号/空格 =========\n",
    "value_cols = colnames[1:]\n",
    "for c in value_cols:\n",
    "    data[c] = (\n",
    "        data[c]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .str.replace(\" \", \"\", regex=False)\n",
    "        .replace({\"\": np.nan, \"nan\": np.nan})\n",
    "    )\n",
    "    data[c] = pd.to_numeric(data[c], errors=\"coerce\")\n",
    "\n",
    "# ========= 频率识别：来自第二行（freq_row） =========\n",
    "# 第0个元素是“频率”文本，后续与各列一一对应\n",
    "freq_map = dict(zip(value_cols, freq_row[1:]))\n",
    "\n",
    "# ========= 缺失处理规则 =========\n",
    "df = data.set_index(\"date\").copy()\n",
    "\n",
    "# 1) 月频：按“当月最后一个已知值”整月铺满；若该月全缺，用上一月值\n",
    "for c in value_cols:\n",
    "    if \"月\" in freq_map.get(c, \"\"):\n",
    "        # 先得到“每月最后一个有效观测值”\n",
    "        monthly_last = df[c].groupby([df.index.year, df.index.month]).apply(lambda s: s.ffill().iloc[-1])\n",
    "        # 生成一个 {year,month} -> 值 的映射\n",
    "        ym_val = {}\n",
    "        for (y, m), v in monthly_last.items():\n",
    "            ym_val[(y, m)] = v\n",
    "        # 回填不存在的月份：用上一月值递推\n",
    "        # 取数据所覆盖的所有 (y,m)\n",
    "        years = sorted(df.index.year.unique())\n",
    "        # 构造有序 (y,m) 序列\n",
    "        ym_all = sorted({(d.year, d.month) for d in df.index})\n",
    "        last_v = np.nan\n",
    "        for y, m in ym_all:\n",
    "            if (y, m) in ym_val and pd.notna(ym_val[(y, m)]):\n",
    "                last_v = ym_val[(y, m)]\n",
    "            else:\n",
    "                ym_val[(y, m)] = last_v\n",
    "        # 按天赋值\n",
    "        df[c] = [ ym_val[(d.year, d.month)] for d in df.index ]\n",
    "\n",
    "# 2) 日频：前向填充，再后向填充（防止开头为 NaN）\n",
    "for c in value_cols:\n",
    "    if \"日\" in freq_map.get(c, \"\"):\n",
    "        df[c] = df[c].ffill().bfill()\n",
    "\n",
    "df = df.reset_index()  # 还原日期列\n",
    "\n",
    "# ========= 导出：原值填充版 =========\n",
    "out_filled = out_dir / \"controls_filled_daily.csv\"\n",
    "df.to_csv(out_filled, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ========= 导出：标准化（z-score）版（便于进模型） =========\n",
    "zdf = df.copy()\n",
    "for c in value_cols:\n",
    "    mu = zdf[c].mean()\n",
    "    sd = zdf[c].std(ddof=1)\n",
    "    if not np.isfinite(sd) or sd == 0:\n",
    "        sd = 1.0\n",
    "    zdf[c] = (zdf[c] - mu) / sd\n",
    "\n",
    "out_z = out_dir / \"controls_filled_daily_zscore.csv\"\n",
    "zdf.to_csv(out_z, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"已导出：\")\n",
    "print(\"-\", out_filled)\n",
    "print(\"-\", out_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac521e5",
   "metadata": {},
   "source": [
    "# 整合数据\n",
    "\n",
    "用类的方式整合数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23e0ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas numpy openpyxl\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Dict, List, Tuple, Union\n",
    "\n",
    "# =========================\n",
    "# 行业配置：把“杂乱参数”收敛成一个对象\n",
    "# =========================\n",
    "@dataclass\n",
    "class IndustryConfig:\n",
    "    # 输出标识\n",
    "    industry_tag: str          # 输出文件名前缀，例如 \"文娱\"\n",
    "    industry_name_for_D: str   # D 文件里 industry 列的取值，例如 \"文娱\"\n",
    "\n",
    "    # Y 宽表里 ETF 对应列名\n",
    "    etf_col_name_in_ywide: str\n",
    "\n",
    "    # xlsx 行业控制变量列名映射\n",
    "    premium_col_name_in_xlsx: str\n",
    "    mom1w_col_name_in_xlsx: str\n",
    "\n",
    "    # 每行业输入文件名（都相对 p_new）\n",
    "    filename_et_daily: str\n",
    "    filename_oi_daily: str\n",
    "    filename_D_single: str\n",
    "\n",
    "    # 可选：如果你未来每行业的 Y 宽表不是统一文件，也可加字段覆盖\n",
    "    filename_y_wide: str = \"etf_y_slog_wide.csv\"\n",
    "\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    统一管理路径、默认参数、IO、缓存，以及 OI/ET/合并面板的生产过程\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        p_base: Union[str, Path],\n",
    "        *,\n",
    "        dir_raw: str = \"原始数据\",\n",
    "        dir_new: str = \"新数据\",\n",
    "        dir_final: str = \"整合数据\",\n",
    "        filename_ctrl_z: str = \"controls_filled_daily_zscore.csv\",\n",
    "        filename_industry_xlsx: str = \"各行业etf控制变量.xlsx\",\n",
    "        sheet_premium: str = \"溢价率（各行业估值水平）\",\n",
    "        sheet_mom1w: str = \"近1周收益率动量\",\n",
    "        d_col: str = \"D_total_norm\",\n",
    "        prefer_halflife: str = \"7\",\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        self.p_base = Path(p_base)\n",
    "        self.p_raw = self.p_base / dir_raw\n",
    "        self.p_new = self.p_base / dir_new\n",
    "        self.p_final = self.p_base / dir_final\n",
    "\n",
    "        self.p_new.mkdir(parents=True, exist_ok=True)\n",
    "        self.p_final.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.filename_ctrl_z = filename_ctrl_z\n",
    "        self.filename_industry_xlsx = filename_industry_xlsx\n",
    "        self.sheet_premium = sheet_premium\n",
    "        self.sheet_mom1w = sheet_mom1w\n",
    "\n",
    "        self.d_col = d_col\n",
    "        self.prefer_halflife = prefer_halflife\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # 简单缓存，避免重复读（尤其是 controls 与 xlsx）\n",
    "        self._cache: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    # ---------- 基础IO ----------\n",
    "    @staticmethod\n",
    "    def _read_csv_any(path: Union[str, Path]) -> pd.DataFrame:\n",
    "        path = str(path)\n",
    "        for enc in [\"utf-8-sig\", \"utf-8\", \"gbk\"]:\n",
    "            try:\n",
    "                return pd.read_csv(path, sep=None, engine=\"python\", encoding=enc)\n",
    "            except Exception:\n",
    "                continue\n",
    "        raise RuntimeError(f\"读取CSV失败：{path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _pick_date_col(df: pd.DataFrame) -> str:\n",
    "        return \"date\" if \"date\" in df.columns else df.columns[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _parse_date_series(s: pd.Series) -> pd.Series:\n",
    "        return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "    def _log(self, *args):\n",
    "        if self.verbose:\n",
    "            print(*args)\n",
    "\n",
    "    # ---------- 列选择策略 ----------\n",
    "    def _pick_et_col(self, df: pd.DataFrame) -> str:\n",
    "        c1 = f\"ET_ema_hl{self.prefer_halflife}\"\n",
    "        if c1 in df.columns:\n",
    "            return c1\n",
    "        ema_cols = sorted([c for c in df.columns if c.startswith(\"ET_ema_hl\")])\n",
    "        if ema_cols:\n",
    "            return ema_cols[0]\n",
    "        for c in [\"ET_z\", \"ET_mean\"]:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        raise ValueError(f\"ET 文件中未找到可用列：{df.columns.tolist()}\")\n",
    "\n",
    "    def _pick_oi_col(self, df: pd.DataFrame) -> str:\n",
    "        c1 = f\"OI_ema_hl{self.prefer_halflife}\"\n",
    "        if c1 in df.columns:\n",
    "            return c1\n",
    "        if \"OI_mean\" in df.columns:\n",
    "            return \"OI_mean\"\n",
    "        z_cols = [c for c in df.columns if c.startswith(\"z_\")]\n",
    "        if z_cols:\n",
    "            # fallback：均值\n",
    "            df[\"_OI_fallback\"] = df[z_cols].mean(axis=1, skipna=True)\n",
    "            return \"_OI_fallback\"\n",
    "        raise ValueError(f\"OI 文件中未找到可用列：{df.columns.tolist()}\")\n",
    "\n",
    "    # ---------- 读取公共输入（带缓存） ----------\n",
    "    def load_controls_z(self) -> pd.DataFrame:\n",
    "        key = \"controls_z\"\n",
    "        if key in self._cache:\n",
    "            return self._cache[key].copy()\n",
    "\n",
    "        path = self.p_new / self.filename_ctrl_z\n",
    "        dc = self._read_csv_any(path)\n",
    "        dc.columns = [str(c).strip() for c in dc.columns]\n",
    "        date_col = self._pick_date_col(dc)\n",
    "        dc[date_col] = self._parse_date_series(dc[date_col])\n",
    "        dc = dc.dropna(subset=[date_col]).rename(columns={date_col: \"date\"})\n",
    "        self._cache[key] = dc.copy()\n",
    "        return dc\n",
    "\n",
    "    def read_industry_col(self, sheet: str, industry_colname: str, varname: str) -> pd.DataFrame:\n",
    "        # xlsx 读取也做缓存：按 sheet 缓存整张表，避免每行业重复读\n",
    "        key = f\"xlsx::{sheet}\"\n",
    "        if key not in self._cache:\n",
    "            xlsx_path = self.p_raw / self.filename_industry_xlsx\n",
    "            df = pd.read_excel(str(xlsx_path), sheet_name=sheet)\n",
    "            df.columns = [str(c).strip() for c in df.columns]\n",
    "            self._cache[key] = df.copy()\n",
    "\n",
    "        df = self._cache[key].copy()\n",
    "        date_col = \"date\" if \"date\" in df.columns else (\"日期\" if \"日期\" in df.columns else df.columns[0])\n",
    "\n",
    "        if industry_colname not in df.columns:\n",
    "            raise ValueError(f\"[{sheet}] 找不到列：{industry_colname}；可选列：{df.columns.tolist()}\")\n",
    "\n",
    "        out = df[[date_col, industry_colname]].copy()\n",
    "        out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[date_col])\n",
    "\n",
    "        out[industry_colname] = (\n",
    "            out[industry_colname].astype(str)\n",
    "            .str.replace(\",\", \"\", regex=False)\n",
    "            .str.replace(\" \", \"\", regex=False)\n",
    "            .replace({\"\": np.nan, \"nan\": np.nan})\n",
    "        )\n",
    "        out[industry_colname] = pd.to_numeric(out[industry_colname], errors=\"coerce\")\n",
    "        out = out.rename(columns={date_col: \"date\", industry_colname: varname})\n",
    "        return out[[\"date\", varname]]\n",
    "\n",
    "    # =========================\n",
    "    # 核心：合并一个行业面板\n",
    "    # =========================\n",
    "    def merge_panel(self, cfg: IndustryConfig, *, export_trading_only: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame, Path, Optional[Path]]:\n",
    "        # ---- Y ----\n",
    "        path_y = self.p_new / cfg.filename_y_wide\n",
    "        dy = self._read_csv_any(path_y)\n",
    "        dy.columns = [str(c).strip() for c in dy.columns]\n",
    "        date_col_y = dy.columns[0]\n",
    "        dy[date_col_y] = self._parse_date_series(dy[date_col_y])\n",
    "        dy = dy.dropna(subset=[date_col_y])\n",
    "\n",
    "        if cfg.etf_col_name_in_ywide not in dy.columns:\n",
    "            raise ValueError(f\"在 {path_y} 中找不到列：{cfg.etf_col_name_in_ywide}\")\n",
    "        dy = dy[[date_col_y, cfg.etf_col_name_in_ywide]].rename(columns={date_col_y: \"date\", cfg.etf_col_name_in_ywide: \"Y\"})\n",
    "\n",
    "        # ---- ET ----\n",
    "        det = self._read_csv_any(self.p_new / cfg.filename_et_daily)\n",
    "        det.columns = [str(c).strip() for c in det.columns]\n",
    "        date_col_et = self._pick_date_col(det)\n",
    "        det[date_col_et] = self._parse_date_series(det[date_col_et])\n",
    "        det = det.dropna(subset=[date_col_et])\n",
    "        et_col = self._pick_et_col(det)\n",
    "        det = det[[date_col_et, et_col]].rename(columns={date_col_et: \"date\", et_col: \"ET\"})\n",
    "\n",
    "        # ---- Controls ----\n",
    "        dc = self.load_controls_z()\n",
    "        ctrl_cols = [c for c in dc.columns if c != \"date\"]\n",
    "        dc = dc[[\"date\"] + ctrl_cols]\n",
    "\n",
    "        # ---- Industry xlsx: premium & mom_1w ----\n",
    "        premium_df = self.read_industry_col(self.sheet_premium, cfg.premium_col_name_in_xlsx, \"premium\")\n",
    "        mom1w_df   = self.read_industry_col(self.sheet_mom1w,   cfg.mom1w_col_name_in_xlsx,   \"mom_1w\")\n",
    "\n",
    "        # ---- Base merge ----\n",
    "        df = dy.merge(det, on=\"date\", how=\"outer\") \\\n",
    "               .merge(dc,  on=\"date\", how=\"outer\") \\\n",
    "               .merge(premium_df, on=\"date\", how=\"outer\") \\\n",
    "               .merge(mom1w_df,   on=\"date\", how=\"outer\")\n",
    "\n",
    "        # ---- OI ----\n",
    "        doi = self._read_csv_any(self.p_new / cfg.filename_oi_daily)\n",
    "        doi.columns = [str(c).strip() for c in doi.columns]\n",
    "        date_col_oi = self._pick_date_col(doi)\n",
    "        doi[date_col_oi] = self._parse_date_series(doi[date_col_oi])\n",
    "        doi = doi.dropna(subset=[date_col_oi])\n",
    "\n",
    "        oi_col = self._pick_oi_col(doi)\n",
    "        doi = doi.groupby(date_col_oi, as_index=False)[oi_col].mean()\n",
    "        doi = doi.rename(columns={date_col_oi: \"date\", oi_col: \"OI\"})\n",
    "        df = df.merge(doi, on=\"date\", how=\"outer\")\n",
    "\n",
    "        # ---- D ----\n",
    "        dD = self._read_csv_any(self.p_new / cfg.filename_D_single)\n",
    "        dD.columns = [str(c).strip() for c in dD.columns]\n",
    "        date_col_D = self._pick_date_col(dD)\n",
    "        dD[date_col_D] = self._parse_date_series(dD[date_col_D])\n",
    "        dD = dD.dropna(subset=[date_col_D])\n",
    "\n",
    "        if \"industry\" in dD.columns:\n",
    "            dD[\"industry\"] = dD[\"industry\"].astype(str).str.strip()\n",
    "            dD = dD[dD[\"industry\"].eq(str(cfg.industry_name_for_D))]\n",
    "\n",
    "        if self.d_col not in dD.columns:\n",
    "            raise ValueError(f\"{cfg.filename_D_single} 中找不到列：{self.d_col}；实际列：{dD.columns.tolist()}\")\n",
    "\n",
    "        dD = dD[[date_col_D, self.d_col]].rename(columns={date_col_D: \"date\", self.d_col: \"D\"})\n",
    "        df = df.merge(dD, on=\"date\", how=\"left\")\n",
    "\n",
    "        # ---- sort & missing report ----\n",
    "        df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "        na_report = pd.concat(\n",
    "            [df.isna().sum().rename(\"na_count\"), df.isna().mean().rename(\"na_ratio\")],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # ---- export ----\n",
    "        out_merged = self.p_final / f\"{cfg.industry_tag}_merged_panel.csv\"\n",
    "        out_na     = self.p_final / f\"{cfg.industry_tag}_merged_missing_report.csv\"\n",
    "        df.to_csv(out_merged, index=False, encoding=\"utf-8-sig\")\n",
    "        na_report.to_csv(out_na, encoding=\"utf-8-sig\")\n",
    "\n",
    "        out_trading = None\n",
    "        if export_trading_only:\n",
    "            out_trading = self.p_final / f\"{cfg.industry_tag}_merged_panel_trading.csv\"\n",
    "            df_trading = df.dropna(subset=[\"Y\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "            df_trading.to_csv(out_trading, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        self._log(\"合并完成：\", cfg.industry_tag, out_merged)\n",
    "        if out_trading:\n",
    "            self._log(\"交易日面板：\", out_trading)\n",
    "\n",
    "        return df, na_report, out_merged, out_trading\n",
    "\n",
    "    # 批量\n",
    "    def run_batch(self, configs: Sequence[IndustryConfig], *, export_trading_only: bool = True) -> List[Tuple[str, Path, Optional[Path]]]:\n",
    "        results = []\n",
    "        for cfg in configs:\n",
    "            _, _, merged_path, trading_path = self.merge_panel(cfg, export_trading_only=export_trading_only)\n",
    "            results.append((cfg.industry_tag, merged_path, trading_path))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3abf7",
   "metadata": {},
   "source": [
    "输出整合好的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd645c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并完成： 文娱 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\文娱_merged_panel.csv\n",
      "交易日面板： C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\文娱_merged_panel_trading.csv\n",
      "合并完成： 动漫 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\动漫_merged_panel.csv\n",
      "交易日面板： C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\动漫_merged_panel_trading.csv\n",
      "合并完成： 家电 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\家电_merged_panel.csv\n",
      "交易日面板： C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\家电_merged_panel_trading.csv\n",
      "合并完成： 汽车 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\汽车_merged_panel.csv\n",
      "交易日面板： C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\汽车_merged_panel_trading.csv\n",
      "合并完成： 餐饮 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\餐饮_merged_panel.csv\n",
      "交易日面板： C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\餐饮_merged_panel_trading.csv\n",
      "合并完成： 旅游 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\旅游_merged_panel.csv\n",
      "交易日面板： C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\旅游_merged_panel_trading.csv\n",
      "文娱 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\文娱_merged_panel.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\文娱_merged_panel_trading.csv\n",
      "动漫 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\动漫_merged_panel.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\动漫_merged_panel_trading.csv\n",
      "家电 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\家电_merged_panel.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\家电_merged_panel_trading.csv\n",
      "汽车 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\汽车_merged_panel.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\汽车_merged_panel_trading.csv\n",
      "餐饮 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\餐饮_merged_panel.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\餐饮_merged_panel_trading.csv\n",
      "旅游 C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\旅游_merged_panel.csv C:\\Users\\17514\\Desktop\\研一\\统计基础\\作业\\大作业\\整合数据\\旅游_merged_panel_trading.csv\n"
     ]
    }
   ],
   "source": [
    "pipe = DataPipeline(\n",
    "    p_base=BASE_DIR,\n",
    "    prefer_halflife=\"7\",      # ET/OI 优先用 *_ema_hl7\n",
    "    d_col=\"D_total_norm\",\n",
    "    verbose=True\n",
    ")\n",
    "configs = [\n",
    "    IndustryConfig(\n",
    "        industry_tag=\"文娱\",\n",
    "        industry_name_for_D=\"文娱\",\n",
    "        etf_col_name_in_ywide=\"华夏中证文娱传媒ETF\",\n",
    "        premium_col_name_in_xlsx=\"华夏中证文娱传媒ETF\",\n",
    "        mom1w_col_name_in_xlsx=\"华夏中证文娱传媒ETF\",\n",
    "        filename_et_daily=\"ET_daily_文娱传媒.csv\",\n",
    "        filename_oi_daily=\"OI_daily_文娱传媒_simple.csv\",\n",
    "        filename_D_single=\"policy_D_daily__文娱.csv\",\n",
    "    ),\n",
    "    # 其余行业照抄一份改字段即可\n",
    "    IndustryConfig(\n",
    "        industry_tag=\"动漫\",\n",
    "        industry_name_for_D=\"动漫\",\n",
    "        etf_col_name_in_ywide=\"华夏中证动漫游戏ETF\",\n",
    "        premium_col_name_in_xlsx=\"华夏中证动漫游戏\",\n",
    "        mom1w_col_name_in_xlsx=\"华夏中证动漫游戏ETF\",\n",
    "        filename_et_daily=\"ET_daily_动漫.csv\",\n",
    "        filename_oi_daily=\"OI_daily_动漫_simple.csv\",\n",
    "        filename_D_single=\"policy_D_daily__动漫游戏.csv\",\n",
    "    ),\n",
    "    IndustryConfig(\n",
    "        industry_tag=\"家电\",\n",
    "        industry_name_for_D=\"家电\",\n",
    "        etf_col_name_in_ywide=\"国泰中证全指家用电器ETF\",\n",
    "        premium_col_name_in_xlsx=\"国泰中证全指家用电器ETF\",\n",
    "        mom1w_col_name_in_xlsx=\"国泰中证全指家用电器ETF\",\n",
    "        filename_et_daily=\"ET_daily_家电.csv\",\n",
    "        filename_oi_daily=\"OI_daily_家电_simple.csv\",\n",
    "        filename_D_single=\"policy_D_daily__家用电器.csv\",\n",
    "    ),\n",
    "    IndustryConfig(\n",
    "        industry_tag=\"汽车\",\n",
    "        industry_name_for_D=\"文娱\",\n",
    "        etf_col_name_in_ywide=\"国泰中证800汽车与零部件ETF\",\n",
    "        premium_col_name_in_xlsx=\"国泰中证800汽车与零部件ETF\",\n",
    "        mom1w_col_name_in_xlsx=\"国泰中证800汽车与零部件ETF\",\n",
    "        filename_et_daily=\"ET_daily_文娱传媒.csv\",\n",
    "        filename_oi_daily=\"OI_daily_文娱传媒_simple.csv\",\n",
    "        filename_D_single=\"policy_D_daily__文娱.csv\",\n",
    "    ),\n",
    "    IndustryConfig(\n",
    "        industry_tag=\"餐饮\",\n",
    "        industry_name_for_D=\"餐饮\",\n",
    "        etf_col_name_in_ywide=\"华宝中证细分食品饮料产业主题ETF\",\n",
    "        premium_col_name_in_xlsx=\"华宝中证细分食品饮料产业主题ETF\",\n",
    "        mom1w_col_name_in_xlsx=\"华宝中证细分食品饮料产业主题ETF\",\n",
    "        filename_et_daily=\"ET_daily_食品饮料.csv\",\n",
    "        filename_oi_daily=\"OI_daily_食品_simple.csv\",\n",
    "        filename_D_single=\"policy_D_daily__餐饮.csv\",\n",
    "    ),\n",
    "    IndustryConfig(\n",
    "        industry_tag=\"旅游\",\n",
    "        industry_name_for_D=\"旅游\",\n",
    "        etf_col_name_in_ywide=\"富国中证旅游主题ETF\",\n",
    "        premium_col_name_in_xlsx=\"富国中证旅游主题ETF\",\n",
    "        mom1w_col_name_in_xlsx=\"富国中证旅游主题ETF\",\n",
    "        filename_et_daily=\"ET_daily_旅游.csv\",\n",
    "        filename_oi_daily=\"OI_daily_旅游_simple.csv\",\n",
    "        filename_D_single=\"policy_D_daily__旅游.csv\",\n",
    "    )\n",
    "]\n",
    "\n",
    "res = pipe.run_batch(configs, export_trading_only=True)\n",
    "for tag, merged_path, trading_path in res:\n",
    "    print(tag, merged_path, trading_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
