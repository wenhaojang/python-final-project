{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "550323ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¤„ç†æ–‡ä»¶: æ–‡æœ¬æ•°æ®\\ã€å®¶ç”µã€‘bk0456_page-1-4.csv\n",
      "æ­£åœ¨å¤„ç†æ–‡ä»¶: æ–‡æœ¬æ•°æ®\\ã€å®¶ç”µï¼ˆç¾çš„ï¼‰ã€‘000333_page-27-215_converted.csv\n",
      "æˆåŠŸåˆå¹¶ 2 ä¸ªæ–‡ä»¶ï¼Œç­›é€‰åå…± 14151 è¡Œæ•°æ®ï¼ˆ05-01 è‡³ 10-31ï¼‰ï¼Œå·²æŒ‰ 'update' æ’åºã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "folder_path = 'æ–‡æœ¬æ•°æ®'\n",
    "name = 'å®¶ç”µ'\n",
    "\n",
    "# åŒ¹é…æ‰€æœ‰ *é£Ÿå“é¥®æ–™*.csv æ–‡ä»¶\n",
    "file_pattern = os.path.join(folder_path, f'*{name}*.csv')\n",
    "file_list = glob.glob(file_pattern)\n",
    "\n",
    "if not file_list:\n",
    "    print(\"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶ï¼\")\n",
    "else:\n",
    "    # æŒ‡å®šéœ€è¦ä¿ç•™çš„åˆ—\n",
    "    required_columns = ['author', 'comments', 'update']\n",
    "    \n",
    "    df_list = []\n",
    "    for file in file_list:\n",
    "        print(f\"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file}\")\n",
    "        df = pd.read_csv(file)\n",
    "        cols_to_keep = [col for col in required_columns if col in df.columns]\n",
    "        df = df[cols_to_keep]\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # åˆå¹¶\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    if 'update' not in merged_df.columns:\n",
    "        raise ValueError(\"æ‰€æœ‰æ–‡ä»¶ä¸­éƒ½ç¼ºå°‘ 'update' åˆ—ï¼\")\n",
    "\n",
    "    # è¡¥å…¨å¹´ä»½ä»¥ä¾¿è§£ææ—¥æœŸï¼ˆå‡è®¾éƒ½æ˜¯å½“å‰å¹´ï¼‰\n",
    "    current_year = datetime.now().year\n",
    "    merged_df['update_full'] = merged_df['update'].astype(str).apply(\n",
    "        lambda x: f\"{current_year}-{x}\" if '-' in x and len(x.split('-')[0]) <= 2 else x\n",
    "    )\n",
    "\n",
    "    # è½¬ä¸º datetime\n",
    "    merged_df['update_date'] = pd.to_datetime(merged_df['update_full'], format='%Y-%m-%d', errors='coerce')\n",
    "    merged_df = merged_df.dropna(subset=['update_date'])\n",
    "\n",
    "    # === æ–°å¢ï¼šç­›é€‰ 05-01 åˆ° 10-31 çš„æ•°æ®ï¼ˆä¸ä¾èµ–å¹´ä»½ï¼‰===\n",
    "    # æå–æœˆå’Œæ—¥ç»„æˆ MM-DD å­—ç¬¦ä¸²\n",
    "    merged_df['mm_dd'] = merged_df['update_date'].dt.strftime('%m-%d')\n",
    "    \n",
    "    # å®šä¹‰èµ·æ­¢\n",
    "    start = '05-01'\n",
    "    end = '10-31'\n",
    "    \n",
    "    # ç­›é€‰ï¼šæ³¨æ„è¿™é‡Œç›´æ¥å­—ç¬¦ä¸²æ¯”è¾ƒæ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºæ ¼å¼å›ºå®šä¸º mm-dd\n",
    "    filtered_df = merged_df[(merged_df['mm_dd'] >= start) & (merged_df['mm_dd'] <= end)].copy()\n",
    "\n",
    "    # æŒ‰ update_date æ’åº\n",
    "    filtered_df = filtered_df.sort_values(by='update_date', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # åªä¿ç•™åŸå§‹åˆ—\n",
    "    final_df = filtered_df[required_columns]\n",
    "\n",
    "    # ä¿å­˜ç»“æœ\n",
    "    output_path = os.path.join(\"æ–‡æœ¬æ•°æ®åˆå¹¶\", f'merged_{name}.csv')\n",
    "    final_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"æˆåŠŸåˆå¹¶ {len(file_list)} ä¸ªæ–‡ä»¶ï¼Œç­›é€‰åå…± {len(final_df)} è¡Œæ•°æ®ï¼ˆ05-01 è‡³ 10-31ï¼‰ï¼Œå·²æŒ‰ 'update' æ’åºã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c985fe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å®Œæˆï¼åŸå§‹ 14151 è¡Œï¼Œç­›é€‰å 552 è¡Œã€‚\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comments</th>\n",
       "      <th>update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n1652137528020204934</td>\n",
       "      <td>ä»Šå¤©å‡ºä¸€éƒ¨åˆ†åšä¸ªTï¼Ÿèƒ½å¦ï¼Œè€å¸ˆä»¬</td>\n",
       "      <td>10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>è‚¡æµ·å‚é’“è¶‹åŠ¿æŠ•èµ„</td>\n",
       "      <td>ç§‘æŠ€å›è½ï¼Œè½¬æˆ˜ä½ä¼°çº¢åˆ©</td>\n",
       "      <td>10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wqxiangfei</td>\n",
       "      <td>è¢«è¿™åƒåœ¾è‚¡å¥—å¥½å¤šå¹´ï¼Œ91ä¹°çš„</td>\n",
       "      <td>10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>è‚¡å‹2369Y861v9</td>\n",
       "      <td>è°ƒæŸ¥ä¸‹ï¼Œè°ç”¨è¿‡å°ç±³ç©ºè°ƒå•Šï¼ŒçœŸçš„èƒ½åˆ¶å†·ä¹ˆï¼Œæ˜¯ä¸æ˜¯åªæ˜¯ä¸ªåªå¹é£çš„ç©ºå£³å•Š</td>\n",
       "      <td>10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aè‚¡20å¹´çš„å·´è²ç‰¹</td>\n",
       "      <td>å·¥ä¸šäº’è”ä¸€å®¶å°æ¹¾å…¬å¸ï¼Œå¸‚å€¼1.5ä¸‡äº¿ï¼ŒåŠ¨ä¸åŠ¨å°±æ¶¨åœï¼ŒåŠ¨ä¸åŠ¨å°±æ¶¨äº”å…­ä¸ªç‚¹ï¼Œä½ æ‰äº”å…­</td>\n",
       "      <td>10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>å¼€å¿ƒå°±å¥½178</td>\n",
       "      <td>ç¾çš„å¼€å§‹ç¬¬äºŒæ³¢ä¸»å‡æµªï¼Œå¥¥åˆ©ç»™</td>\n",
       "      <td>10-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>è‚¡å‹2369Y861v9</td>\n",
       "      <td>è¿™æ ·çš„å¤§ç›˜èµ°åŠ¿ç¾çš„ä¸æ¶¨ï¼Œé‚£ä¹ˆå¤§ç›˜ä¸‹è¡Œçš„æ—¶å€™è¿˜ä¼šå›åˆ°72é™„è¿‘</td>\n",
       "      <td>10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>å¿—è¿œå•†è´¸æœ‰é™å…¬å¸</td>\n",
       "      <td>ç¡¬è¦äºæŸä¸€åˆ†</td>\n",
       "      <td>10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>é˜³å…‰çš„è®¸å“²</td>\n",
       "      <td>æ¶å¿ƒï¼ŒçœŸæ˜¯æ¶å¿ƒï¼Œéƒ½ä¸èƒ½çº¢ä¸€ä¸‹</td>\n",
       "      <td>10-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>é—²äº‘é‡é¹¤wws</td>\n",
       "      <td>ç¾çš„å¦æœ‰æ‹…å½“ï¼ŒæŠ¤å¤§ç›˜ï¼Œæ˜¯è‚¡æŒ‡æœŸè´§æ ‡çš„åˆè¦å…¼é¡¾Hè‚¡ï¼ŒçœŸæ­£åæ˜ å®¶ç”µå—è¶‹åŠ¿çš„æ˜¯æ ¼åŠ›ä¸æµ·</td>\n",
       "      <td>10-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 author                                  comments update\n",
       "0  n1652137528020204934                          ä»Šå¤©å‡ºä¸€éƒ¨åˆ†åšä¸ªTï¼Ÿèƒ½å¦ï¼Œè€å¸ˆä»¬  10-31\n",
       "1              è‚¡æµ·å‚é’“è¶‹åŠ¿æŠ•èµ„                               ç§‘æŠ€å›è½ï¼Œè½¬æˆ˜ä½ä¼°çº¢åˆ©  10-31\n",
       "2            wqxiangfei                            è¢«è¿™åƒåœ¾è‚¡å¥—å¥½å¤šå¹´ï¼Œ91ä¹°çš„  10-31\n",
       "3          è‚¡å‹2369Y861v9         è°ƒæŸ¥ä¸‹ï¼Œè°ç”¨è¿‡å°ç±³ç©ºè°ƒå•Šï¼ŒçœŸçš„èƒ½åˆ¶å†·ä¹ˆï¼Œæ˜¯ä¸æ˜¯åªæ˜¯ä¸ªåªå¹é£çš„ç©ºå£³å•Š  10-30\n",
       "4             Aè‚¡20å¹´çš„å·´è²ç‰¹  å·¥ä¸šäº’è”ä¸€å®¶å°æ¹¾å…¬å¸ï¼Œå¸‚å€¼1.5ä¸‡äº¿ï¼ŒåŠ¨ä¸åŠ¨å°±æ¶¨åœï¼ŒåŠ¨ä¸åŠ¨å°±æ¶¨äº”å…­ä¸ªç‚¹ï¼Œä½ æ‰äº”å…­  10-30\n",
       "5               å¼€å¿ƒå°±å¥½178                            ç¾çš„å¼€å§‹ç¬¬äºŒæ³¢ä¸»å‡æµªï¼Œå¥¥åˆ©ç»™  10-30\n",
       "6          è‚¡å‹2369Y861v9             è¿™æ ·çš„å¤§ç›˜èµ°åŠ¿ç¾çš„ä¸æ¶¨ï¼Œé‚£ä¹ˆå¤§ç›˜ä¸‹è¡Œçš„æ—¶å€™è¿˜ä¼šå›åˆ°72é™„è¿‘  10-29\n",
       "7              å¿—è¿œå•†è´¸æœ‰é™å…¬å¸                                    ç¡¬è¦äºæŸä¸€åˆ†  10-29\n",
       "8                 é˜³å…‰çš„è®¸å“²                            æ¶å¿ƒï¼ŒçœŸæ˜¯æ¶å¿ƒï¼Œéƒ½ä¸èƒ½çº¢ä¸€ä¸‹  10-29\n",
       "9               é—²äº‘é‡é¹¤wws  ç¾çš„å¦æœ‰æ‹…å½“ï¼ŒæŠ¤å¤§ç›˜ï¼Œæ˜¯è‚¡æŒ‡æœŸè´§æ ‡çš„åˆè¦å…¼é¡¾Hè‚¡ï¼ŒçœŸæ­£åæ˜ å®¶ç”µå—è¶‹åŠ¿çš„æ˜¯æ ¼åŠ›ä¸æµ·  10-28"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "name = 'å®¶ç”µ' # åŠ¨æ¼«ï¼Œå®¶ç”µï¼Œæ—…æ¸¸ï¼Œæ±½è½¦ï¼Œé£Ÿå“é¥®æ–™ï¼Œæ–‡å¨±ä¼ åª’\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_csv(f'æ–‡æœ¬æ•°æ®åˆå¹¶/merged_{name}.csv')\n",
    "\n",
    "# å¯é€‰ï¼šç¡®ä¿åªåŒ…å«å¿…è¦åˆ—ï¼ˆæ ¹æ®ä½ å±•ç¤ºçš„æ•°æ®ï¼‰\n",
    "df = df[['author', 'comments', 'update']].copy()\n",
    "\n",
    "# æŒ‰ update åˆ†ç»„ï¼Œæ¯ç»„å–æœ€å¤šå‰ 3 è¡Œ\n",
    "df_limited = df.groupby('update').head(3).reset_index(drop=True)\n",
    "\n",
    "# ä¿å­˜ç»“æœï¼ˆå¯é€‰ï¼‰\n",
    "df_limited.to_csv(f'æ–‡æœ¬æ•°æ®åˆå¹¶/merged_{name}_top3_per_day.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"å¤„ç†å®Œæˆï¼åŸå§‹ {len(df)} è¡Œï¼Œç­›é€‰å {len(df_limited)} è¡Œã€‚\")\n",
    "df_limited.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4fe4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv('æ–‡æœ¬æ•°æ®/ã€å®¶ç”µï¼ˆç¾çš„ï¼‰ã€‘000333_page-27-215.csv')\n",
    "\n",
    "def to_mmdd(date_str):\n",
    "    match = re.match(r'(\\d{1,2})æœˆ(\\d{1,2})æ—¥', date_str)\n",
    "    if match:\n",
    "        return f\"{int(match.group(1)):02d}-{int(match.group(2)):02d}\"\n",
    "    return None\n",
    "\n",
    "df['update'] = df['update'].apply(to_mmdd)\n",
    "df.to_csv('æ–‡æœ¬æ•°æ®/ã€å®¶ç”µï¼ˆç¾çš„ï¼‰ã€‘000333_page-27-215_converted.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45d9169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ æ‰¾åˆ° 6 ä¸ªæƒ…æ„Ÿåˆ†æç»“æœæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...\n",
      "ğŸ“„ å¤„ç†: merged_åŠ¨æ¼«_top3_per_day_sentiment.csv\n",
      "  âœ… å·²ä¿å­˜: merged_åŠ¨æ¼«_top3_per_day_sentiment_daily_avg.csv\n",
      "ğŸ“„ å¤„ç†: merged_å®¶ç”µ_top3_per_day_sentiment.csv\n",
      "  âœ… å·²ä¿å­˜: merged_å®¶ç”µ_top3_per_day_sentiment_daily_avg.csv\n",
      "ğŸ“„ å¤„ç†: merged_æ–‡å¨±ä¼ åª’_top3_per_day_sentiment.csv\n",
      "  âœ… å·²ä¿å­˜: merged_æ–‡å¨±ä¼ åª’_top3_per_day_sentiment_daily_avg.csv\n",
      "ğŸ“„ å¤„ç†: merged_æ—…æ¸¸_top3_per_day_sentiment.csv\n",
      "  âœ… å·²ä¿å­˜: merged_æ—…æ¸¸_top3_per_day_sentiment_daily_avg.csv\n",
      "ğŸ“„ å¤„ç†: merged_æ±½è½¦_top3_per_day_sentiment.csv\n",
      "  âœ… å·²ä¿å­˜: merged_æ±½è½¦_top3_per_day_sentiment_daily_avg.csv\n",
      "ğŸ“„ å¤„ç†: merged_é£Ÿå“é¥®æ–™_top3_per_day_sentiment.csv\n",
      "  âœ… å·²ä¿å­˜: merged_é£Ÿå“é¥®æ–™_top3_per_day_sentiment_daily_avg.csv\n",
      "\n",
      "ğŸ‰ æ‰€æœ‰æ–‡ä»¶çš„æ—¥å‡æƒ…æ„Ÿåˆ†å·²ç”Ÿæˆå®Œæ¯•ï¼\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# è®¾ç½®è¾“å…¥è¾“å‡ºç›®å½•\n",
    "input_dir = Path(\"output\")\n",
    "output_dir = input_dir  # ä¹Ÿå¯ä»¥è®¾ä¸ºå…¶ä»–ç›®å½•ï¼Œå¦‚ Path(\"output/daily_avg\")\n",
    "\n",
    "# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼ˆå¦‚æœå¦è®¾è·¯å¾„ï¼‰\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# æŸ¥æ‰¾æ‰€æœ‰ _sentiment.csv æ–‡ä»¶\n",
    "sentiment_files = list(input_dir.glob(\"*_sentiment.csv\"))\n",
    "\n",
    "if not sentiment_files:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°ä»»ä½• *_sentiment.csv æ–‡ä»¶\")\n",
    "else:\n",
    "    print(f\"ğŸ“‚ æ‰¾åˆ° {len(sentiment_files)} ä¸ªæƒ…æ„Ÿåˆ†æç»“æœæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...\")\n",
    "\n",
    "for file_path in sentiment_files:\n",
    "    try:\n",
    "        print(f\"ğŸ“„ å¤„ç†: {file_path.name}\")\n",
    "        \n",
    "        # è¯»å–æ–‡ä»¶\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        \n",
    "        # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨\n",
    "        if 'update' not in df.columns or 'sentiment_score' not in df.columns:\n",
    "            print(f\"  âš ï¸ è·³è¿‡ {file_path.name}ï¼šç¼ºå°‘ 'update' æˆ– 'sentiment_score' åˆ—\")\n",
    "            continue\n",
    "\n",
    "        # ç¡®ä¿ update æ˜¯å­—ç¬¦ä¸²\n",
    "        df['update'] = df['update'].astype(str)\n",
    "        \n",
    "        # æŒ‰ update åˆ†ç»„æ±‚å¹³å‡\n",
    "        daily_avg = df.groupby('update')['sentiment_score'].mean().round(2).reset_index()\n",
    "        \n",
    "        # ï¼ˆå¯é€‰ï¼‰æŒ‰æ—¥æœŸæ’åºï¼ˆå‡è®¾æ˜¯åŒä¸€å¹´å†…çš„ MM-DD æ ¼å¼ï¼‰\n",
    "        # å¦‚æœè·¨å¹´ï¼Œå»ºè®®è¡¥å……å¹´ä»½åå†æ’åº\n",
    "        daily_avg = daily_avg.sort_values('update').reset_index(drop=True)\n",
    "        \n",
    "        # æ„é€ æ–°æ–‡ä»¶åï¼šåŸå + _daily_avg.csv\n",
    "        new_filename = file_path.stem + \"_daily_avg.csv\"\n",
    "        output_file = output_dir / new_filename\n",
    "        \n",
    "        # ä¿å­˜\n",
    "        daily_avg.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"  âœ… å·²ä¿å­˜: {output_file.name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ å¤„ç† {file_path.name} æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ æ‰€æœ‰æ–‡ä»¶çš„æ—¥å‡æƒ…æ„Ÿåˆ†å·²ç”Ÿæˆå®Œæ¯•ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45215bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38df3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
